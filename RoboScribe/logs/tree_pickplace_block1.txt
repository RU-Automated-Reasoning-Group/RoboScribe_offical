
-------------------------------------------
current attempt: 0
current task contains:
S0-->S1-->
          
S0: NA / False / num:0.0 
S1: <Environment Reward> / False / num:29.8 

-------------------------------------------

starting training to current 1 function list
get reward 0.03 for reward function: 
 <Environment Reward>
before opt reward function: 
 ( obs[1]<=0.09573931060731411 )

max([ obs[1]-0.09573931060731411 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.03169776126742363 )

max([ obs[1]-0.03169776126742363 ])
before opt reward function: 
 ( obs[1]<=0.09573931060731411 )

max([ obs[1]-0.09573931060731411 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.03169776126742363 )

max([ obs[1]-0.03169776126742363 ])
before opt reward function: 
 ( obs[1]<=0.09573931060731411 )

max([ obs[1]-0.09573931060731411 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.03169776126742363 )

max([ obs[1]-0.03169776126742363 ])
before opt reward function: 
 ( obs[1]<=0.09573931060731411 )

max([ obs[1]-0.09573931060731411 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.03169776126742363 )

max([ obs[1]-0.03169776126742363 ])
before opt reward function: 
 ( obs[1]<=0.09573931060731411 )

max([ obs[1]-0.09573931060731411 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.03169776126742363 )

max([ obs[1]-0.03169776126742363 ])
before opt reward function: 
 ( obs[1]<=0.09573931060731411 )

max([ obs[1]-0.09573931060731411 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.03169776126742363 )

max([ obs[1]-0.03169776126742363 ])
before opt reward function: 
 ( obs[1]<=0.09573931060731411 )

max([ obs[1]-0.09573931060731411 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.03169776126742363 )

max([ obs[1]-0.03169776126742363 ])
before opt reward function: 
 ( obs[1]<=0.09573931060731411 )

max([ obs[1]-0.09573931060731411 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.03169776126742363 )

max([ obs[1]-0.03169776126742363 ])
before opt reward function: 
 ( obs[1]<=0.09573931060731411 )

max([ obs[1]-0.09573931060731411 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.03169776126742363 )

max([ obs[1]-0.03169776126742363 ])
before opt reward function: 
 ( obs[1]<=0.09573931060731411 )

max([ obs[1]-0.09573931060731411 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.03169776126742363 )

max([ obs[1]-0.03169776126742363 ])
accuracy of reward functoin is 1.0
current attempt fail
get new reward function: 
( obs[1]<=0.03169776126742363 )

max([ obs[1]-0.03169776126742363 ])
get new intermediate index: 
[26 27 27 27 25 28 29 31 34 32 19 25 22 23 32 22 22 34 44 26 29 24 25 32
 26 28 29 23 26 33 29 26 35 27 25 30 26 22 32 27 29 28 20 26 20 33 29 27
 28 23 29 31 33 34 24 24 22 33 24 24 34 31 28 28 26 30 25 25 19 38 27 30
 24 25 28 24 24 28 30 45 24 27 22 28 24 33 27 27 31 24 27 23 27 29 26 30
 25 23 31 22]

-------------------------------------------
current attempt: 1
current task contains:
S0-->S2-->
          
S0: NA / False / num:0.0 
S2: ( obs[1]<=0.03169776126742363 )

max([ obs[1]-0.03169776126742363 ]) / False / num:27.49 

-------------------------------------------

starting training to current 1 function list
get reward 0.04 for reward function: 
 ( obs[1]<=0.03169776126742363 )

max([ obs[1]-0.03169776126742363 ])
before opt reward function: 
 ( obs[0]<=0.047792257741093636 )

max([ obs[0]-0.047792257741093636 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.025865789502859116 )

max([ obs[0]-0.025865789502859116 ])
before opt reward function: 
 ( obs[0]<=0.047792257741093636 )

max([ obs[0]-0.047792257741093636 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.025865789502859116 )

max([ obs[0]-0.025865789502859116 ])
before opt reward function: 
 ( obs[1]<=0.07398639805614948 )

max([ obs[1]-0.07398639805614948 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.047848839312791824 )

max([ obs[1]-0.047848839312791824 ])
before opt reward function: 
 ( obs[1]<=0.07398639805614948 )

max([ obs[1]-0.07398639805614948 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.047848839312791824 )

max([ obs[1]-0.047848839312791824 ])
before opt reward function: 
 ( obs[1]<=0.07398639805614948 )

max([ obs[1]-0.07398639805614948 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.047848839312791824 )

max([ obs[1]-0.047848839312791824 ])
before opt reward function: 
 ( obs[1]<=0.07398639805614948 )

max([ obs[1]-0.07398639805614948 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.047848839312791824 )

max([ obs[1]-0.047848839312791824 ])
before opt reward function: 
 ( obs[0]<=0.047792257741093636 )

max([ obs[0]-0.047792257741093636 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.025865789502859116 )

max([ obs[0]-0.025865789502859116 ])
before opt reward function: 
 ( obs[0]<=0.047792257741093636 )

max([ obs[0]-0.047792257741093636 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.025865789502859116 )

max([ obs[0]-0.025865789502859116 ])
before opt reward function: 
 ( obs[1]<=0.07398639805614948 )

max([ obs[1]-0.07398639805614948 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.047848839312791824 )

max([ obs[1]-0.047848839312791824 ])
before opt reward function: 
 ( obs[1]<=0.07398639805614948 )

max([ obs[1]-0.07398639805614948 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.047848839312791824 )

max([ obs[1]-0.047848839312791824 ])
accuracy of reward functoin is 1.0
current attempt fail
get new reward function: 
( obs[0]<=0.025865789502859116 )

max([ obs[0]-0.025865789502859116 ])
get new intermediate index: 
[ 8  8 11  8 10 11 11 10 11 10  8  8  7  8 10  7  8 11 11 10 11  8  8 10
  9 11 11  8  8 12  8  8 10 12  8 11 10  7 10 11  8  8  7  8  8 10 12 11
 11  8  8 11 10 12  8 10  8 12  8 10 11 10 11 11  9  8 10  7  7 11 10  9
  8 10  8  8 10  8 11 10  8 11  8 11 10 10  8 11 10  8 10  8  9 10  8 11
  8  8 11  8]

-------------------------------------------
current attempt: 2
current task contains:
S0-->S3-->S2-->
               
S0: NA / False / num:0.0 
S3: ( obs[0]<=0.025865789502859116 )

max([ obs[0]-0.025865789502859116 ]) / False / num:9.38 
S2: ( obs[1]<=0.03169776126742363 )

max([ obs[1]-0.03169776126742363 ]) / False / num:27.49 

-------------------------------------------

starting training to current 1 function list
get reward 1.0 for reward function: 
 ( obs[0]<=0.025865789502859116 )

max([ obs[0]-0.025865789502859116 ])
current attempt success and store skill

-------------------------------------------
current attempt: 3
current task contains:
S0==>S3-->S2-->
               
S0: NA / True / num:0.0 
S3: ( obs[0]<=0.025865789502859116 )

max([ obs[0]-0.025865789502859116 ]) / False / num:9.38 
S2: ( obs[1]<=0.03169776126742363 )

max([ obs[1]-0.03169776126742363 ]) / False / num:27.49 

-------------------------------------------

starting training to current 1 function list
get reward 0.99 for reward function: 
 ( obs[1]<=0.03169776126742363 )

max([ obs[1]-0.03169776126742363 ])
current attempt success and store skill
evaluation success rate: 0.992
finish and complete for 2 skills
