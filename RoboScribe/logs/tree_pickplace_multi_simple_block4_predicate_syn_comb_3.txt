
-------------------------------------------
current attempt: 0
current task contains:
S0-->S1-->
          
S0: NA / False / num:0.0 
S1: <Environment Reward> / False / num:57.04 

-------------------------------------------

starting training to current 1 function list
get reward 0.0 for reward function: 
 <Environment Reward>
before opt reward function: 
 ( obs[2]<=0.05101541988551617 )

max([ obs[2]-0.05101541988551617 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.05101541988551617 )

max([ obs[2]-0.05101541988551617 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.05101541988551617 )

max([ obs[2]-0.05101541988551617 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.05101541988551617 )

max([ obs[2]-0.05101541988551617 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.05101541988551617 )

max([ obs[2]-0.05101541988551617 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.05101541988551617 )

max([ obs[2]-0.05101541988551617 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.05101541988551617 )

max([ obs[2]-0.05101541988551617 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.05101541988551617 )

max([ obs[2]-0.05101541988551617 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.05101541988551617 )

max([ obs[2]-0.05101541988551617 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.05101541988551617 )

max([ obs[2]-0.05101541988551617 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
accuracy of reward functoin is 1.0
current attempt fail
get new reward function: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
get new intermediate index: 
[21 27 29 25 25 26 23 26 26 25 28 25 23 24 23 23 24 23 23 22 29 24 20 24
 26 22 23 21 23 29 22 25 24 26 28 22 22 25 25 23 22 23 21 24 24 26 28 28
 23 24 24 28 25 24 30 23 25 26 21 20 26 25 26 28 21 23 22 23 24 23 24 22
 22 24 25 22 26 21 25 22 24 32 22 25 25 23 26 25 24 24 24 26 24 29 25 23
 23 25 27 23]

-------------------------------------------
current attempt: 1
current task contains:
S0-->S2-->
          
S0: NA / False / num:0.0 
S2: ( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ]) / False / num:24.36 

-------------------------------------------

starting training to current 1 function list
get reward 0.0 for reward function: 
 ( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.06007746234536171 )

max([ obs[2]-0.06007746234536171 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.036173827946186066 )

max([ obs[2]-0.036173827946186066 ])
before opt reward function: 
 ( obs[4]>0.02390612749150023 )

max([ 0.02390612749150023+1e-05-obs[4] ])

 comb with cover rule to update reward function and get: 
( obs[4]>0.04764920845627785 )

max([ 0.04764920845627785+1e-05-obs[4] ])
before opt reward function: 
 ( obs[0]<=0.0605367012321949 )

max([ obs[0]-0.0605367012321949 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.027050569653511047 )

max([ obs[0]-0.027050569653511047 ])
before opt reward function: 
 ( obs[4]>0.02390612749150023 )

max([ 0.02390612749150023+1e-05-obs[4] ])

 comb with cover rule to update reward function and get: 
( obs[4]>0.04764920845627785 )

max([ 0.04764920845627785+1e-05-obs[4] ])
before opt reward function: 
 ( obs[2]<=0.06007746234536171 )

max([ obs[2]-0.06007746234536171 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.036173827946186066 )

max([ obs[2]-0.036173827946186066 ])
before opt reward function: 
 ( obs[0]<=0.0605367012321949 )

max([ obs[0]-0.0605367012321949 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.027050569653511047 )

max([ obs[0]-0.027050569653511047 ])
before opt reward function: 
 ( obs[2]<=0.06007746234536171 )

max([ obs[2]-0.06007746234536171 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.036173827946186066 )

max([ obs[2]-0.036173827946186066 ])
before opt reward function: 
 ( obs[0]<=0.0605367012321949 )

max([ obs[0]-0.0605367012321949 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.027050569653511047 )

max([ obs[0]-0.027050569653511047 ])
before opt reward function: 
 ( obs[0]<=0.0605367012321949 )

max([ obs[0]-0.0605367012321949 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.027050569653511047 )

max([ obs[0]-0.027050569653511047 ])
before opt reward function: 
 ( obs[2]<=0.06007746234536171 )

max([ obs[2]-0.06007746234536171 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.036173827946186066 )

max([ obs[2]-0.036173827946186066 ])
accuracy of reward functoin is 1.0
current attempt fail
get new reward function: 
( obs[0]<=0.027050569653511047 )

max([ obs[0]-0.027050569653511047 ])
get new intermediate index: 
[7 7 8 7 7 8 7 7 8 7 8 7 8 7 7 8 7 8 8 7 8 7 7 8 8 8 7 7 8 8 8 7 8 7 9 7 8
 7 8 7 7 7 8 7 8 8 8 8 7 8 8 8 7 8 8 8 7 8 8 7 8 8 7 8 7 8 7 7 7 7 7 7 7 8
 7 7 8 7 7 8 8 9 8 7 8 7 9 8 8 8 8 8 8 9 8 8 8 7 8 7]

-------------------------------------------
current attempt: 2
current task contains:
S0-->S3-->S2-->
               
S0: NA / False / num:0.0 
S3: ( obs[0]<=0.027050569653511047 )

max([ obs[0]-0.027050569653511047 ]) / False / num:7.59 
S2: ( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ]) / False / num:24.36 

-------------------------------------------

starting training to current 1 function list
get reward 0.12 for reward function: 
 ( obs[0]<=0.027050569653511047 )

max([ obs[0]-0.027050569653511047 ])
