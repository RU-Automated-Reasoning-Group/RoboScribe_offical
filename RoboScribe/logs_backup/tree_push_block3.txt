
-------------------------------------------
current attempt: 0
current task contains:
S0-->S1-->
          
S0: NA / False / num:0.0 
S1: <Environment Reward> / False / num:39.99 

-------------------------------------------

starting training to current 1 function list
get reward 0.0 for reward function: 
 <Environment Reward>
before opt reward function: 
 ( obs[3]<=0.05448709987103939 )

max([ obs[3]-0.05448709987103939 ])

 comb with cover rule to update reward function and get: 
( obs[3]<=0.048738728701877065 )

max([ obs[3]-0.048738728701877065 ])
before opt reward function: 
 ( obs[2]<=0.06208432838320732 )

max([ obs[2]-0.06208432838320732 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.0484494125365534 )

max([ obs[2]-0.0484494125365534 ])
before opt reward function: 
 ( obs[2]<=0.06208432838320732 )

max([ obs[2]-0.06208432838320732 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.0484494125365534 )

max([ obs[2]-0.0484494125365534 ])
before opt reward function: 
 ( obs[3]<=0.05448709987103939 )

max([ obs[3]-0.05448709987103939 ])

 comb with cover rule to update reward function and get: 
( obs[3]<=0.048738728701877065 )

max([ obs[3]-0.048738728701877065 ])
before opt reward function: 
 ( obs[2]<=0.06208432838320732 )

max([ obs[2]-0.06208432838320732 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.0484494125365534 )

max([ obs[2]-0.0484494125365534 ])
before opt reward function: 
 ( obs[3]<=0.05448709987103939 )

max([ obs[3]-0.05448709987103939 ])

 comb with cover rule to update reward function and get: 
( obs[3]<=0.048738728701877065 )

max([ obs[3]-0.048738728701877065 ])
before opt reward function: 
 ( obs[2]<=0.06208432838320732 )

max([ obs[2]-0.06208432838320732 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.0484494125365534 )

max([ obs[2]-0.0484494125365534 ])
before opt reward function: 
 ( obs[2]<=0.06208432838320732 )

max([ obs[2]-0.06208432838320732 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.0484494125365534 )

max([ obs[2]-0.0484494125365534 ])
before opt reward function: 
 ( obs[3]<=0.05448709987103939 )

max([ obs[3]-0.05448709987103939 ])

 comb with cover rule to update reward function and get: 
( obs[3]<=0.048738728701877065 )

max([ obs[3]-0.048738728701877065 ])
before opt reward function: 
 ( obs[3]<=0.05448709987103939 )

max([ obs[3]-0.05448709987103939 ])

 comb with cover rule to update reward function and get: 
( obs[3]<=0.048738728701877065 )

max([ obs[3]-0.048738728701877065 ])
accuracy of reward functoin is 1.0
current attempt fail
get new reward function: 
( obs[2]<=0.0484494125365534 )

max([ obs[2]-0.0484494125365534 ])
get new intermediate index: 
[20 14 15 19 19 16 17 25 17 19 22 15 35 16 17 18 19 19 21 20 18 19 23 17
 16 16 34 20 18 15 18 15 18 14 36 32 18 30 19 19 18 16 22 17 23 34 29 20
 13 18 20 15 15 20 12 18 16 17 16 19 16 19 15 18 19 15 16 17 14 22 16 20
 15 17 17 16 18 13 24 21 16 19 17 26 21 16  4 18 17  3 27 15 15 18 16 16
 18 21 26 22]

-------------------------------------------
current attempt: 1
current task contains:
S0-->S2-->
          
S0: NA / False / num:0.0 
S2: ( obs[2]<=0.0484494125365534 )

max([ obs[2]-0.0484494125365534 ]) / False / num:18.75 

-------------------------------------------

starting training to current 1 function list
get reward 0.0 for reward function: 
 ( obs[2]<=0.0484494125365534 )

max([ obs[2]-0.0484494125365534 ])
before opt reward function: 
 ( obs[2]<=0.07661232352256775 )

max([ obs[2]-0.07661232352256775 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
before opt reward function: 
 ( obs[2]<=0.07661232352256775 )

max([ obs[2]-0.07661232352256775 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
before opt reward function: 
 ( obs[2]<=0.07661232352256775 )

max([ obs[2]-0.07661232352256775 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
before opt reward function: 
 ( obs[0]<=0.09596912190318108 )

max([ obs[0]-0.09596912190318108 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.08322491611795602 )

max([ obs[0]-0.08322491611795602 ])
before opt reward function: 
 ( obs[2]<=0.07661232352256775 )

max([ obs[2]-0.07661232352256775 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
before opt reward function: 
 ( obs[0]<=0.09596912190318108 )

max([ obs[0]-0.09596912190318108 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.08322491611795602 )

max([ obs[0]-0.08322491611795602 ])
before opt reward function: 
 ( obs[2]<=0.07661232352256775 )

max([ obs[2]-0.07661232352256775 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
before opt reward function: 
 ( obs[2]<=0.07661232352256775 )

max([ obs[2]-0.07661232352256775 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
before opt reward function: 
 ( obs[2]<=0.07661232352256775 )

max([ obs[2]-0.07661232352256775 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
before opt reward function: 
 ( obs[0]<=0.09596912190318108 )

max([ obs[0]-0.09596912190318108 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.08322491611795602 )

max([ obs[0]-0.08322491611795602 ])
accuracy of reward functoin is 1.0
current attempt fail
get new reward function: 
( obs[0]<=0.08322491611795602 )

max([ obs[0]-0.08322491611795602 ])
get new intermediate index: 
[ 7 10  7  6  7  8  7  7  7  8  7  6 29 12  7  7  8 12  6  7  6  7  7  6
  7  7 28  6  7  7 11  6  7  8 33  7  7  8  7  6  6  6  8 11  7 30  6  7
  6  7  7  6 11  7  9  6  7  7  7  7  8  7  8 15  8  6  7  6 10  8  7  7
 10 12 11 11  8  6  8  7  6 12 11  7  7  8  2  6  7  1  7  6  8  8  6 11
  7  7 21  8]

-------------------------------------------
current attempt: 2
current task contains:
S0-->S3-->S2-->
               
S0: NA / False / num:0.0 
S3: ( obs[0]<=0.08322491611795602 )

max([ obs[0]-0.08322491611795602 ]) / False / num:8.57 
S2: ( obs[2]<=0.0484494125365534 )

max([ obs[2]-0.0484494125365534 ]) / False / num:18.75 

-------------------------------------------

starting training to current 1 function list
get reward 1.0 for reward function: 
 ( obs[0]<=0.08322491611795602 )

max([ obs[0]-0.08322491611795602 ])
current attempt success and store skill

-------------------------------------------
current attempt: 3
current task contains:
S0==>S3-->S2-->
               
S0: NA / True / num:0.0 
S3: ( obs[0]<=0.08322491611795602 )

max([ obs[0]-0.08322491611795602 ]) / False / num:8.57 
S2: ( obs[2]<=0.0484494125365534 )

max([ obs[2]-0.0484494125365534 ]) / False / num:18.75 

-------------------------------------------

starting training to current 1 function list
get reward 0.0 for reward function: 
 ( obs[2]<=0.0484494125365534 )

max([ obs[2]-0.0484494125365534 ])
before opt reward function: 
 ( obs[2]<=0.08014123141765594 )

max([ obs[2]-0.08014123141765594 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
before opt reward function: 
 ( obs[2]<=0.08014123141765594 )

max([ obs[2]-0.08014123141765594 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
before opt reward function: 
 ( obs[2]<=0.08014123141765594 )

max([ obs[2]-0.08014123141765594 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
before opt reward function: 
 ( obs[2]<=0.08014123141765594 )

max([ obs[2]-0.08014123141765594 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
before opt reward function: 
 ( obs[2]<=0.08014123141765594 )

max([ obs[2]-0.08014123141765594 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
before opt reward function: 
 ( obs[2]<=0.08014123141765594 )

max([ obs[2]-0.08014123141765594 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
before opt reward function: 
 ( obs[2]<=0.08014123141765594 )

max([ obs[2]-0.08014123141765594 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
before opt reward function: 
 ( obs[2]<=0.08014123141765594 )

max([ obs[2]-0.08014123141765594 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
before opt reward function: 
 ( obs[2]<=0.08014123141765594 )

max([ obs[2]-0.08014123141765594 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
before opt reward function: 
 ( obs[2]<=0.08014123141765594 )

max([ obs[2]-0.08014123141765594 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
accuracy of reward functoin is 1.0
current attempt fail
get new reward function: 
( obs[2]<=0.07374040411006658 )

max([ obs[2]-0.07374040411006658 ])
get new intermediate index: 
[19 13 14 18 18 15 16 22 16 18 21 14 34 15 15 16 18 18 18 19 17 18 17 16
 15 15 33 19 17 14 16 14 17  0 20 21 17 28 18 18 17 15 21 16 21 23 28 19
  0 16 19 14 14 19  0 17 15 16 15 18 14 18 14  0 18 14 15 15 13 21 15 18
 14 15 16 15 17  0 22 20 15 17 16 23 20  0  0 17 16  0 25 14 14 17 15 15
 17 20 25 21]

 comb with cover rule to update reward function and get: 
( obs[2]<=0.07374040411006658 )
 const:
( obs[0]<=0.08322491611795602 )

max([ obs[2]-0.07374040411006658 ])
 const:
max([ obs[0]-0.08322491611795602 ])

-------------------------------------------
current attempt: 4
current task contains:
S0==>S3-->S4-->S2-->
                    
S0: NA / True / num:0.0 
S3: ( obs[0]<=0.08322491611795602 )

max([ obs[0]-0.08322491611795602 ]) / False / num:8.57 
S4: ( obs[2]<=0.07374040411006658 )
 const:
( obs[0]<=0.08322491611795602 )

max([ obs[2]-0.07374040411006658 ])
 const:
max([ obs[0]-0.08322491611795602 ]) / False / num:16.31 
S2: ( obs[2]<=0.0484494125365534 )

max([ obs[2]-0.0484494125365534 ]) / False / num:18.75 

-------------------------------------------

starting training to current 1 function list
get reward 0.09 for reward function: 
 ( obs[2]<=0.07374040411006658 )
 const:
( obs[0]<=0.08322491611795602 )

max([ obs[2]-0.07374040411006658 ])
 const:
max([ obs[0]-0.08322491611795602 ])
before opt reward function: 
 ( obs[5]<=1.6954089403152466 and obs[5]>1.303832471370697 ) or
( obs[5]>1.6954089403152466 )

max([ obs[5]-1.6954089403152466 , 1.303832471370697+1e-05-obs[5] ]) *
max([ 1.6954089403152466+1e-05-obs[5] ])

 comb with cover rule to update reward function and get: 
( obs[5]>1.3127900842831464 and obs[5]<=1.3127900842831464 ) or
( obs[5]>1.7602987542072566 )

max([ 1.3127900842831464+1e-05-obs[5] , obs[5]-1.3127900842831464 ]) *
max([ 1.7602987542072566+1e-05-obs[5] ])
before opt reward function: 
 ( obs[5]<=1.6954089403152466 and obs[5]>1.303832471370697 ) or
( obs[5]>1.6954089403152466 )

max([ obs[5]-1.6954089403152466 , 1.303832471370697+1e-05-obs[5] ]) *
max([ 1.6954089403152466+1e-05-obs[5] ])

 comb with cover rule to update reward function and get: 
( obs[5]>1.3127900842831464 and obs[5]<=1.3127900842831464 ) or
( obs[5]>1.7602987542072566 )

max([ 1.3127900842831464+1e-05-obs[5] , obs[5]-1.3127900842831464 ]) *
max([ 1.7602987542072566+1e-05-obs[5] ])
before opt reward function: 
 ( obs[5]<=1.6954089403152466 and obs[5]>1.303832471370697 ) or
( obs[5]>1.6954089403152466 )

max([ obs[5]-1.6954089403152466 , 1.303832471370697+1e-05-obs[5] ]) *
max([ 1.6954089403152466+1e-05-obs[5] ])

 comb with cover rule to update reward function and get: 
( obs[5]>1.3127900842831464 and obs[5]<=1.3127900842831464 ) or
( obs[5]>1.7602987542072566 )

max([ 1.3127900842831464+1e-05-obs[5] , obs[5]-1.3127900842831464 ]) *
max([ 1.7602987542072566+1e-05-obs[5] ])
before opt reward function: 
 ( obs[5]<=1.6954089403152466 and obs[5]>1.303832471370697 ) or
( obs[5]>1.6954089403152466 )

max([ obs[5]-1.6954089403152466 , 1.303832471370697+1e-05-obs[5] ]) *
max([ 1.6954089403152466+1e-05-obs[5] ])

 comb with cover rule to update reward function and get: 
( obs[5]>1.3127900842831464 and obs[5]<=1.3127900842831464 ) or
( obs[5]>1.7602987542072566 )

max([ 1.3127900842831464+1e-05-obs[5] , obs[5]-1.3127900842831464 ]) *
max([ 1.7602987542072566+1e-05-obs[5] ])
before opt reward function: 
 ( obs[5]<=1.6954089403152466 and obs[5]>1.303832471370697 ) or
( obs[5]>1.6954089403152466 )

max([ obs[5]-1.6954089403152466 , 1.303832471370697+1e-05-obs[5] ]) *
max([ 1.6954089403152466+1e-05-obs[5] ])

 comb with cover rule to update reward function and get: 
( obs[5]>1.3127900842831464 and obs[5]<=1.3127900842831464 ) or
( obs[5]>1.7602987542072566 )

max([ 1.3127900842831464+1e-05-obs[5] , obs[5]-1.3127900842831464 ]) *
max([ 1.7602987542072566+1e-05-obs[5] ])
before opt reward function: 
 ( obs[5]<=1.6954089403152466 and obs[5]>1.303832471370697 ) or
( obs[5]>1.6954089403152466 )

max([ obs[5]-1.6954089403152466 , 1.303832471370697+1e-05-obs[5] ]) *
max([ 1.6954089403152466+1e-05-obs[5] ])

 comb with cover rule to update reward function and get: 
( obs[5]>1.3127900842831464 and obs[5]<=1.3127900842831464 ) or
( obs[5]>1.7602987542072566 )

max([ 1.3127900842831464+1e-05-obs[5] , obs[5]-1.3127900842831464 ]) *
max([ 1.7602987542072566+1e-05-obs[5] ])
before opt reward function: 
 ( obs[5]<=1.6954089403152466 and obs[5]>1.303832471370697 ) or
( obs[5]>1.6954089403152466 )

max([ obs[5]-1.6954089403152466 , 1.303832471370697+1e-05-obs[5] ]) *
max([ 1.6954089403152466+1e-05-obs[5] ])

 comb with cover rule to update reward function and get: 
( obs[5]>1.3127900842831464 and obs[5]<=1.3127900842831464 ) or
( obs[5]>1.7602987542072566 )

max([ 1.3127900842831464+1e-05-obs[5] , obs[5]-1.3127900842831464 ]) *
max([ 1.7602987542072566+1e-05-obs[5] ])
before opt reward function: 
 ( obs[5]<=1.6954089403152466 and obs[5]>1.303832471370697 ) or
( obs[5]>1.6954089403152466 )

max([ obs[5]-1.6954089403152466 , 1.303832471370697+1e-05-obs[5] ]) *
max([ 1.6954089403152466+1e-05-obs[5] ])

 comb with cover rule to update reward function and get: 
( obs[5]>1.3127900842831464 and obs[5]<=1.3127900842831464 ) or
( obs[5]>1.7602987542072566 )

max([ 1.3127900842831464+1e-05-obs[5] , obs[5]-1.3127900842831464 ]) *
max([ 1.7602987542072566+1e-05-obs[5] ])
before opt reward function: 
 ( obs[5]<=1.6954089403152466 and obs[5]>1.303832471370697 ) or
( obs[5]>1.6954089403152466 )

max([ obs[5]-1.6954089403152466 , 1.303832471370697+1e-05-obs[5] ]) *
max([ 1.6954089403152466+1e-05-obs[5] ])

 comb with cover rule to update reward function and get: 
( obs[5]>1.3127900842831464 and obs[5]<=1.3127900842831464 ) or
( obs[5]>1.7602987542072566 )

max([ 1.3127900842831464+1e-05-obs[5] , obs[5]-1.3127900842831464 ]) *
max([ 1.7602987542072566+1e-05-obs[5] ])
before opt reward function: 
 ( obs[5]<=1.6954089403152466 and obs[5]>1.303832471370697 ) or
( obs[5]>1.6954089403152466 )

max([ obs[5]-1.6954089403152466 , 1.303832471370697+1e-05-obs[5] ]) *
max([ 1.6954089403152466+1e-05-obs[5] ])

 comb with cover rule to update reward function and get: 
( obs[5]>1.3127900842831464 and obs[5]<=1.3127900842831464 ) or
( obs[5]>1.7602987542072566 )

max([ 1.3127900842831464+1e-05-obs[5] , obs[5]-1.3127900842831464 ]) *
max([ 1.7602987542072566+1e-05-obs[5] ])
accuracy of reward functoin is 0.9990439770554493
current attempt fail
get new reward function: 
( obs[5]>1.7602987542072566 )

max([ 1.7602987542072566+1e-05-obs[5] ])
get new intermediate index: 
[ 8  5  9  8  8  9  8  8  7  9  9  8 25  7  9  8  8  8  8  8  8  8  8  7
  8  9 24  8  8  9  7  8  8  0  7 21  8  9  9  7  7  7  9  7  9 23 23  9
  0  8  9  7  7  9  0  7  8  9  9  9  8  8  7  0  9  8  7  7  0  9  7  8
  6  8  7  7  9  0  9  9  8  8  7  8  9  0  0  8  7  0  9  8  6  8  7  7
  9  8  8  9]

 comb with cover rule to update reward function and get: 
( obs[5]>1.7602987542072566 )
 const:
( obs[0]<=0.08322491611795602 )

max([ 1.7602987542072566+1e-05-obs[5] ])
 const:
max([ obs[0]-0.08322491611795602 ])

-------------------------------------------
current attempt: 5
current task contains:
S0==>S3-->S5-->S4-->S2-->
                         
S0: NA / True / num:0.0 
S3: ( obs[0]<=0.08322491611795602 )

max([ obs[0]-0.08322491611795602 ]) / False / num:8.57 
S5: ( obs[5]>1.7602987542072566 )
 const:
( obs[0]<=0.08322491611795602 )

max([ 1.7602987542072566+1e-05-obs[5] ])
 const:
max([ obs[0]-0.08322491611795602 ]) / False / num:8.01 
S4: ( obs[2]<=0.07374040411006658 )
 const:
( obs[0]<=0.08322491611795602 )

max([ obs[2]-0.07374040411006658 ])
 const:
max([ obs[0]-0.08322491611795602 ]) / False / num:16.31 
S2: ( obs[2]<=0.0484494125365534 )

max([ obs[2]-0.0484494125365534 ]) / False / num:18.75 

-------------------------------------------

starting training to current 1 function list
get reward 0.99 for reward function: 
 ( obs[5]>1.7602987542072566 )
 const:
( obs[0]<=0.08322491611795602 )

max([ 1.7602987542072566+1e-05-obs[5] ])
 const:
max([ obs[0]-0.08322491611795602 ])
current attempt success and store skill

-------------------------------------------
current attempt: 6
current task contains:
S0==>S3==>S5-->S4-->S2-->
                         
S0: NA / True / num:0.0 
S3: ( obs[0]<=0.08322491611795602 )

max([ obs[0]-0.08322491611795602 ]) / True / num:8.57 
S5: ( obs[5]>1.7602987542072566 )
 const:
( obs[0]<=0.08322491611795602 )

max([ 1.7602987542072566+1e-05-obs[5] ])
 const:
max([ obs[0]-0.08322491611795602 ]) / False / num:8.01 
S4: ( obs[2]<=0.07374040411006658 )
 const:
( obs[0]<=0.08322491611795602 )

max([ obs[2]-0.07374040411006658 ])
 const:
max([ obs[0]-0.08322491611795602 ]) / False / num:16.31 
S2: ( obs[2]<=0.0484494125365534 )

max([ obs[2]-0.0484494125365534 ]) / False / num:18.75 

-------------------------------------------

starting training to current 1 function list
get reward 0.91 for reward function: 
 ( obs[2]<=0.07374040411006658 )
 const:
( obs[0]<=0.08322491611795602 )

max([ obs[2]-0.07374040411006658 ])
 const:
max([ obs[0]-0.08322491611795602 ])
current attempt success and store skill

-------------------------------------------
current attempt: 7
current task contains:
S0==>S3==>S5==>S4-->S2-->
                         
S0: NA / True / num:0.0 
S3: ( obs[0]<=0.08322491611795602 )

max([ obs[0]-0.08322491611795602 ]) / True / num:8.57 
S5: ( obs[5]>1.7602987542072566 )
 const:
( obs[0]<=0.08322491611795602 )

max([ 1.7602987542072566+1e-05-obs[5] ])
 const:
max([ obs[0]-0.08322491611795602 ]) / True / num:8.01 
S4: ( obs[2]<=0.07374040411006658 )
 const:
( obs[0]<=0.08322491611795602 )

max([ obs[2]-0.07374040411006658 ])
 const:
max([ obs[0]-0.08322491611795602 ]) / False / num:16.31 
S2: ( obs[2]<=0.0484494125365534 )

max([ obs[2]-0.0484494125365534 ]) / False / num:18.75 

-------------------------------------------

starting training to current 1 function list
get reward 0.62 for reward function: 
 ( obs[2]<=0.0484494125365534 )

max([ obs[2]-0.0484494125365534 ])
finish and complete for 11 skills
