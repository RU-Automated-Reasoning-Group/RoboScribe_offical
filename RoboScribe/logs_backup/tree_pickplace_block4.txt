
-------------------------------------------
current attempt: 0
current task contains:
S0-->S1-->
          
S0: NA / False / num:0.0 
S1: <Environment Reward> / False / num:57.04 

-------------------------------------------

starting training to current 1 function list
get reward 0.0 for reward function: 
 <Environment Reward>
before opt reward function: 
 ( obs[2]<=0.06017734296619892 )

max([ obs[2]-0.06017734296619892 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.06017734296619892 )

max([ obs[2]-0.06017734296619892 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.06017734296619892 )

max([ obs[2]-0.06017734296619892 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.06017734296619892 )

max([ obs[2]-0.06017734296619892 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.06017734296619892 )

max([ obs[2]-0.06017734296619892 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.06017734296619892 )

max([ obs[2]-0.06017734296619892 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.06017734296619892 )

max([ obs[2]-0.06017734296619892 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.06017734296619892 )

max([ obs[2]-0.06017734296619892 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.06017734296619892 )

max([ obs[2]-0.06017734296619892 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.06017734296619892 )

max([ obs[2]-0.06017734296619892 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
accuracy of reward functoin is 1.0
current attempt fail
get new reward function: 
( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
get new intermediate index: 
[21 27 29 25 25 26 23 26 26 25 28 25 23 24 23 23 24 23 23 22 29 24 20 24
 26 22 23 21 23 29 22 25 24 26 28 22 22 25 25 23 22 23 21 24 24 26 28 28
 23 24 24 28 25 24 30 23 25 26 21 20 26 25 26 28 21 23 22 23 24 23 24 22
 22 24 25 22 26 21 25 22 24 32 22 25 25 23 26 25 24 24 24 26 24 29 25 23
 23 25 27 23]

-------------------------------------------
current attempt: 1
current task contains:
S0-->S2-->
          
S0: NA / False / num:0.0 
S2: ( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ]) / False / num:24.36 

-------------------------------------------

starting training to current 1 function list
get reward 0.0 for reward function: 
 ( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
before opt reward function: 
 ( obs[2]<=0.06102767214179039 )

max([ obs[2]-0.06102767214179039 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.036173827946186066 )

max([ obs[2]-0.036173827946186066 ])
before opt reward function: 
 ( obs[2]<=0.06102767214179039 )

max([ obs[2]-0.06102767214179039 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.036173827946186066 )

max([ obs[2]-0.036173827946186066 ])
before opt reward function: 
 ( obs[2]<=0.06102767214179039 )

max([ obs[2]-0.06102767214179039 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.036173827946186066 )

max([ obs[2]-0.036173827946186066 ])
before opt reward function: 
 ( obs[2]<=0.06102767214179039 )

max([ obs[2]-0.06102767214179039 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.036173827946186066 )

max([ obs[2]-0.036173827946186066 ])
before opt reward function: 
 ( obs[2]<=0.06102767214179039 )

max([ obs[2]-0.06102767214179039 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.036173827946186066 )

max([ obs[2]-0.036173827946186066 ])
before opt reward function: 
 ( obs[2]<=0.06102767214179039 )

max([ obs[2]-0.06102767214179039 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.036173827946186066 )

max([ obs[2]-0.036173827946186066 ])
before opt reward function: 
 ( obs[0]<=0.08720812201499939 )

max([ obs[0]-0.08720812201499939 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.027050569653511047 )

max([ obs[0]-0.027050569653511047 ])
before opt reward function: 
 ( obs[0]<=0.08720812201499939 )

max([ obs[0]-0.08720812201499939 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.027050569653511047 )

max([ obs[0]-0.027050569653511047 ])
before opt reward function: 
 ( obs[0]<=0.08720812201499939 )

max([ obs[0]-0.08720812201499939 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.027050569653511047 )

max([ obs[0]-0.027050569653511047 ])
before opt reward function: 
 ( obs[2]<=0.06102767214179039 )

max([ obs[2]-0.06102767214179039 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.036173827946186066 )

max([ obs[2]-0.036173827946186066 ])
accuracy of reward functoin is 1.0
current attempt fail
get new reward function: 
( obs[0]<=0.027050569653511047 )

max([ obs[0]-0.027050569653511047 ])
get new intermediate index: 
[7 7 8 7 7 8 7 7 8 7 8 7 8 7 7 8 7 8 8 7 8 7 7 8 8 8 7 7 8 8 8 7 8 7 9 7 8
 7 8 7 7 7 8 7 8 8 8 8 7 8 8 8 7 8 8 8 7 8 8 7 8 8 7 8 7 8 7 7 7 7 7 7 7 8
 7 7 8 7 7 8 8 9 8 7 8 7 9 8 8 8 8 8 8 9 8 8 8 7 8 7]

-------------------------------------------
current attempt: 2
current task contains:
S0-->S3-->S2-->
               
S0: NA / False / num:0.0 
S3: ( obs[0]<=0.027050569653511047 )

max([ obs[0]-0.027050569653511047 ]) / False / num:7.59 
S2: ( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ]) / False / num:24.36 

-------------------------------------------

starting training to current 1 function list
get reward 0.98 for reward function: 
 ( obs[0]<=0.027050569653511047 )

max([ obs[0]-0.027050569653511047 ])
current attempt success and store skill

-------------------------------------------
current attempt: 3
current task contains:
S0==>S3-->S2-->
               
S0: NA / True / num:0.0 
S3: ( obs[0]<=0.027050569653511047 )

max([ obs[0]-0.027050569653511047 ]) / False / num:7.59 
S2: ( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ]) / False / num:24.36 

-------------------------------------------

starting training to current 1 function list
get reward 0.95 for reward function: 
 ( obs[2]<=0.027381058782339096 )

max([ obs[2]-0.027381058782339096 ])
current attempt success and store skill
evaluation success rate: 0.04
