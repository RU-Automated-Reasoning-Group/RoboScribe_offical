
-------------------------------------------
current attempt: 0
current task contains:
S0-->S1-->
          
S0: NA / False / num:0.0 
S1: <Environment Reward> / False / num:65.94 

-------------------------------------------

starting training to current 1 function list
get reward 0.0 for reward function: 
 <Environment Reward>
before opt reward function: 
 ( obs[1]<=0.027740309946238995 )

max([ obs[1]-0.027740309946238995 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.016201762482523918 )

max([ obs[1]-0.016201762482523918 ])
before opt reward function: 
 ( obs[1]<=0.027740309946238995 )

max([ obs[1]-0.027740309946238995 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.016201762482523918 )

max([ obs[1]-0.016201762482523918 ])
before opt reward function: 
 ( obs[3]<=0.07410933263599873 )

max([ obs[3]-0.07410933263599873 ])

 comb with cover rule to update reward function and get: 
( obs[3]<=0.03224998340010643 )

max([ obs[3]-0.03224998340010643 ])
before opt reward function: 
 ( obs[1]<=0.027740309946238995 )

max([ obs[1]-0.027740309946238995 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.016201762482523918 )

max([ obs[1]-0.016201762482523918 ])
before opt reward function: 
 ( obs[2]<=0.05330922640860081 )

max([ obs[2]-0.05330922640860081 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.022553522139787674 )

max([ obs[2]-0.022553522139787674 ])
before opt reward function: 
 ( obs[3]<=0.07410933263599873 )

max([ obs[3]-0.07410933263599873 ])

 comb with cover rule to update reward function and get: 
( obs[3]<=0.03224998340010643 )

max([ obs[3]-0.03224998340010643 ])
before opt reward function: 
 ( obs[1]<=0.027740309946238995 )

max([ obs[1]-0.027740309946238995 ])

 comb with cover rule to update reward function and get: 
( obs[1]<=0.016201762482523918 )

max([ obs[1]-0.016201762482523918 ])
before opt reward function: 
 ( obs[3]<=0.07410933263599873 )

max([ obs[3]-0.07410933263599873 ])

 comb with cover rule to update reward function and get: 
( obs[3]<=0.03224998340010643 )

max([ obs[3]-0.03224998340010643 ])
before opt reward function: 
 ( obs[2]<=0.05330922640860081 )

max([ obs[2]-0.05330922640860081 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.022553522139787674 )

max([ obs[2]-0.022553522139787674 ])
before opt reward function: 
 ( obs[3]<=0.07410933263599873 )

max([ obs[3]-0.07410933263599873 ])

 comb with cover rule to update reward function and get: 
( obs[3]<=0.03224998340010643 )

max([ obs[3]-0.03224998340010643 ])
accuracy of reward functoin is 1.0
current attempt fail
get new reward function: 
( obs[2]<=0.022553522139787674 )

max([ obs[2]-0.022553522139787674 ])
get new intermediate index: 
[24 26 24 29 26 24 23 24 27 30 26 26 24 24 31 32 24 23 27 31 27 28 25 26
 27 28 24 23 29 22 26 24 27 31 25 27 30 25 30 26 29 22 29 23 26 26 26 28
 21 28 25 31 30 23 27 27 29 22 24 30 24 26 28 27 25 28 28 25 28 28 28 24
 30 26 30 23 26 27 27 28 28 24 26 27 25 28 26 29 29 23 47 27 23 32 25 26
 29 27 24 29]

-------------------------------------------
current attempt: 1
current task contains:
S0-->S2-->
          
S0: NA / False / num:0.0 
S2: ( obs[2]<=0.022553522139787674 )

max([ obs[2]-0.022553522139787674 ]) / False / num:26.71 

-------------------------------------------

starting training to current 1 function list
get reward 0.0 for reward function: 
 ( obs[2]<=0.022553522139787674 )

max([ obs[2]-0.022553522139787674 ])
before opt reward function: 
 ( obs[2]<=0.057558005675673485 )

max([ obs[2]-0.057558005675673485 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.03307705000042915 )

max([ obs[2]-0.03307705000042915 ])
before opt reward function: 
 ( obs[2]<=0.057558005675673485 )

max([ obs[2]-0.057558005675673485 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.03307705000042915 )

max([ obs[2]-0.03307705000042915 ])
before opt reward function: 
 ( obs[2]<=0.057558005675673485 )

max([ obs[2]-0.057558005675673485 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.03307705000042915 )

max([ obs[2]-0.03307705000042915 ])
before opt reward function: 
 ( obs[2]<=0.057558005675673485 )

max([ obs[2]-0.057558005675673485 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.03307705000042915 )

max([ obs[2]-0.03307705000042915 ])
before opt reward function: 
 ( obs[2]<=0.057558005675673485 )

max([ obs[2]-0.057558005675673485 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.03307705000042915 )

max([ obs[2]-0.03307705000042915 ])
before opt reward function: 
 ( obs[2]<=0.057558005675673485 )

max([ obs[2]-0.057558005675673485 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.03307705000042915 )

max([ obs[2]-0.03307705000042915 ])
before opt reward function: 
 ( obs[2]<=0.057558005675673485 )

max([ obs[2]-0.057558005675673485 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.03307705000042915 )

max([ obs[2]-0.03307705000042915 ])
before opt reward function: 
 ( obs[2]<=0.057558005675673485 )

max([ obs[2]-0.057558005675673485 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.03307705000042915 )

max([ obs[2]-0.03307705000042915 ])
before opt reward function: 
 ( obs[2]<=0.057558005675673485 )

max([ obs[2]-0.057558005675673485 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.03307705000042915 )

max([ obs[2]-0.03307705000042915 ])
before opt reward function: 
 ( obs[2]<=0.057558005675673485 )

max([ obs[2]-0.057558005675673485 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.03307705000042915 )

max([ obs[2]-0.03307705000042915 ])
accuracy of reward functoin is 1.0
current attempt fail
get new reward function: 
( obs[2]<=0.03307705000042915 )

max([ obs[2]-0.03307705000042915 ])
get new intermediate index: 
[23 24 23 28 25 22 22 23 25 28 24 24 23 23 30 29 23 22 25 30 25 27 24 24
 25 27 23 22 28 21 24 23 25 28 24 24 29 23 29 25 27 21 27 22 24 24 25 26
 20 27 24 29 28 21 26 25 28 21 23 28 22 25 27 26 23 27 27 24 27 27 27 22
 29 25 29 22 25 26 26 27 27 22 25 26 24 27 25 26 27 21 23 26 22 31 24 25
 28 25 23 27]

-------------------------------------------
current attempt: 2
current task contains:
S0-->S3-->S2-->
               
S0: NA / False / num:0.0 
S3: ( obs[2]<=0.03307705000042915 )

max([ obs[2]-0.03307705000042915 ]) / False / num:25.09 
S2: ( obs[2]<=0.022553522139787674 )

max([ obs[2]-0.022553522139787674 ]) / False / num:26.71 

-------------------------------------------

starting training to current 1 function list
get reward 0.0 for reward function: 
 ( obs[2]<=0.03307705000042915 )

max([ obs[2]-0.03307705000042915 ])
before opt reward function: 
 ( obs[2]<=0.07578790746629238 )

max([ obs[2]-0.07578790746629238 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.051235657185316086 )

max([ obs[2]-0.051235657185316086 ])
before opt reward function: 
 ( obs[2]<=0.07578790746629238 )

max([ obs[2]-0.07578790746629238 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.051235657185316086 )

max([ obs[2]-0.051235657185316086 ])
before opt reward function: 
 ( obs[0]<=0.040868005715310574 )

max([ obs[0]-0.040868005715310574 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.027777640148997307 )

max([ obs[0]-0.027777640148997307 ])
before opt reward function: 
 ( obs[0]<=0.040868005715310574 )

max([ obs[0]-0.040868005715310574 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.027777640148997307 )

max([ obs[0]-0.027777640148997307 ])
before opt reward function: 
 ( obs[2]<=0.07578790746629238 )

max([ obs[2]-0.07578790746629238 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.051235657185316086 )

max([ obs[2]-0.051235657185316086 ])
before opt reward function: 
 ( obs[2]<=0.07578790746629238 )

max([ obs[2]-0.07578790746629238 ])

 comb with cover rule to update reward function and get: 
( obs[2]<=0.051235657185316086 )

max([ obs[2]-0.051235657185316086 ])
before opt reward function: 
 ( obs[0]<=0.040868005715310574 )

max([ obs[0]-0.040868005715310574 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.027777640148997307 )

max([ obs[0]-0.027777640148997307 ])
before opt reward function: 
 ( obs[0]<=0.040868005715310574 )

max([ obs[0]-0.040868005715310574 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.027777640148997307 )

max([ obs[0]-0.027777640148997307 ])
before opt reward function: 
 ( obs[0]<=0.040868005715310574 )

max([ obs[0]-0.040868005715310574 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.027777640148997307 )

max([ obs[0]-0.027777640148997307 ])
before opt reward function: 
 ( obs[0]<=0.040868005715310574 )

max([ obs[0]-0.040868005715310574 ])

 comb with cover rule to update reward function and get: 
( obs[0]<=0.027777640148997307 )

max([ obs[0]-0.027777640148997307 ])
accuracy of reward functoin is 1.0
current attempt fail
get new reward function: 
( obs[0]<=0.027777640148997307 )

max([ obs[0]-0.027777640148997307 ])
get new intermediate index: 
[8 8 8 8 7 7 8 8 7 8 7 7 7 7 9 8 7 8 7 8 8 8 7 7 8 7 7 8 8 8 7 8 8 8 8 7 8
 7 8 8 7 8 7 7 8 8 8 9 7 7 8 9 8 7 8 8 8 7 7 8 8 8 8 7 7 8 7 8 8 8 8 7 8 7
 8 8 7 7 8 7 8 7 7 8 7 8 7 8 8 7 8 7 7 9 8 7 8 7 8 9]

-------------------------------------------
current attempt: 3
current task contains:
S0-->S4-->S3-->S2-->
                    
S0: NA / False / num:0.0 
S4: ( obs[0]<=0.027777640148997307 )

max([ obs[0]-0.027777640148997307 ]) / False / num:7.64 
S3: ( obs[2]<=0.03307705000042915 )

max([ obs[2]-0.03307705000042915 ]) / False / num:25.09 
S2: ( obs[2]<=0.022553522139787674 )

max([ obs[2]-0.022553522139787674 ]) / False / num:26.71 

-------------------------------------------

starting training to current 1 function list
get reward 0.96 for reward function: 
 ( obs[0]<=0.027777640148997307 )

max([ obs[0]-0.027777640148997307 ])
current attempt success and store skill

-------------------------------------------
current attempt: 4
current task contains:
S0==>S4-->S3-->S2-->
                    
S0: NA / True / num:0.0 
S4: ( obs[0]<=0.027777640148997307 )

max([ obs[0]-0.027777640148997307 ]) / False / num:7.64 
S3: ( obs[2]<=0.03307705000042915 )

max([ obs[2]-0.03307705000042915 ]) / False / num:25.09 
S2: ( obs[2]<=0.022553522139787674 )

max([ obs[2]-0.022553522139787674 ]) / False / num:26.71 

-------------------------------------------

starting training to current 1 function list
get reward 0.99 for reward function: 
 ( obs[2]<=0.03307705000042915 )

max([ obs[2]-0.03307705000042915 ])
current attempt success and store skill

-------------------------------------------
current attempt: 5
current task contains:
S0==>S4==>S3-->S2-->
                    
S0: NA / True / num:0.0 
S4: ( obs[0]<=0.027777640148997307 )

max([ obs[0]-0.027777640148997307 ]) / True / num:7.64 
S3: ( obs[2]<=0.03307705000042915 )

max([ obs[2]-0.03307705000042915 ]) / False / num:25.09 
S2: ( obs[2]<=0.022553522139787674 )

max([ obs[2]-0.022553522139787674 ]) / False / num:26.71 

-------------------------------------------

starting training to current 1 function list
get reward 0.97 for reward function: 
 ( obs[2]<=0.022553522139787674 )

max([ obs[2]-0.022553522139787674 ])
current attempt success and store skill
evaluation success rate: 0.0